{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em vez de substituir cada palavra por um número, iremos identificar as palavras como vetores em um espaço de n-dimensões.\n",
    "\n",
    "O processo de Embedding é fazer com que um conjunto de palavras ou palavras associadas formem vetores em um espaço multidimensional\n",
    "\n",
    "Nesse semana de curso, será usado o dataset do IMDB, para podermos construir um classificador de críticas em relação a um filme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalando os dataset do tensorflow\n",
    "#!pip install --upgrade pip\n",
    "#!pip install -q tensorflow-datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import io\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "#print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = pd.read_csv(r'D:\\Cursos_DS\\NLP_coursera\\Data\\IMDB_reviews\\IMDB Dataset.csv' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tamanho do Dataset\n",
    "len(imdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "imdb.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    25000\n",
       "negative    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Apply function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (209614605.py, line 103)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [50]\u001b[1;36m\u001b[0m\n\u001b[1;33m    text = re.sub(r\"\\\",\"\", text)\u001b[0m\n\u001b[1;37m                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "# Remove as tags de HTML\n",
    "def _remove_tags_html(text):\n",
    "    text = re.sub('<[^<]+?>', '', str(text))\n",
    "    return text\n",
    "\n",
    "\n",
    "# Remove e-mail\n",
    "def _remove_email(text):\n",
    "    text = ' '.join([w for w in text.split() if '@' not in w])\n",
    "    return text\n",
    "\n",
    "# Remove os Emojis\n",
    "def _remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                u\"\\U00002702-\\U000027B0\"\n",
    "                u\"\\U000024C2-\\U0001F251\"\n",
    "                u\"\\U0001f926-\\U0001f937\"\n",
    "                u'\\U00010000-\\U0010ffff'\n",
    "                u\"\\u200d\"\n",
    "                u\"\\u2640-\\u2642\"\n",
    "                u\"\\u2600-\\u2B55\"\n",
    "                u\"\\u23cf\"\n",
    "                u\"\\u23e9\"\n",
    "                u\"\\u231a\"\n",
    "                u\"\\u3030\"\n",
    "                u\"\\ufe0f\"\n",
    "                \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "def _sub_char_especial_normal(text):\n",
    "    # Substitui caracteres especiais por letras com acentos\n",
    "\n",
    "    # Letra minúsculas\n",
    "    text = re.sub(r\"&aacute;\", \"á\", text)\n",
    "    text = re.sub(r\"&atilde;\", \"ã\", text)\n",
    "    text = re.sub(r\"&agrave;\", \"à\", text)\n",
    "    text = re.sub(r\"&acirc;\", \"â\", text)\n",
    "    text = re.sub(r\"&aring;\", \"a\", text)\n",
    "    text = re.sub(r\"&auml;\", \"a\", text)\n",
    "    text = re.sub(r\"&aelig;\", \"ae\", text)\n",
    "    text = re.sub(r\"&eacute;\", \"é\", text)\n",
    "    text = re.sub(r\"&ecirc;\", \"ê\", text)\n",
    "    text = re.sub(r\"&egrave;\", \"è\", text)\n",
    "    text = re.sub(r\"&euml;\", \"e\", text)\n",
    "    text = re.sub(r\"&iacute;\", \"í\", text)\n",
    "    text = re.sub(r\"&icirc;\", \"î\", text)\n",
    "    text = re.sub(r\"&iuml;\", \"i\", text)\n",
    "    text = re.sub(r\"&oacute;\", \"ó\", text)\n",
    "    text = re.sub(r\"&otilde;\", \"õ\", text)\n",
    "    text = re.sub(r\"&ocirc;\", \"ô\", text)\n",
    "    text = re.sub(r\"&ograve;\", \"ò\", text)\n",
    "    text = re.sub(r\"&uacute;\", \"ú\", text)\n",
    "    text = re.sub(r\"&ucirc;\", \"û\", text)\n",
    "    text = re.sub(r\"&ugrave;\", \"ù\", text)\n",
    "    text = re.sub(r\"&uuml;\", \"u\", text)\n",
    "    text = re.sub(r\"&ccedil;\", \"ç\", text)\n",
    "    text = re.sub(r\"&ntilde;\", \"não\", text)\n",
    "\n",
    "    # Letras maiúsculas\n",
    "    text = re.sub(r\"&Aacute;\", \"Á\", text)\n",
    "    text = re.sub(r\"&Atilde;\",\"Ã\", text) # Letra maiúscula\n",
    "    text = re.sub(r\"&Agrave;\", \"À\", text)\n",
    "    text = re.sub(r\"&Acirc;\", \"Â\", text)\n",
    "    text = re.sub(r\"&Aring;\", \"A\", text)\n",
    "    text = re.sub(r\"&Auml;\", \"A\", text)\n",
    "    text = re.sub(r\"&Aelig;\", \"AE\", text)\n",
    "    text = re.sub(r\"&Eacute;\", \"É\", text)\n",
    "    text = re.sub(r\"&Ecirc;\", \"Ê\", text)\n",
    "    text = re.sub(r\"&Egrave;\", \"È\", text)\n",
    "    text = re.sub(r\"&Euml;\", \"E\", text)\n",
    "    text = re.sub(r\"&Iacute;\", \"Í\", text)\n",
    "    text = re.sub(r\"&Icirc;\", \"Î\", text)\n",
    "    text = re.sub(r\"&Iuml;\", \"I\", text)\n",
    "    text = re.sub(r\"&Oacute;\", \"Ó\", text)\n",
    "    text = re.sub(r\"&Otilde;\", \"Õ\", text)\n",
    "    text = re.sub(r\"&Ocirc;\", \"Ô\", text)\n",
    "    text = re.sub(r\"&Ograve;\", \"Ò\", text)\n",
    "    text = re.sub(r\"&Uacute;\", \"Ú\", text)\n",
    "    text = re.sub(r\"&Ucirc;\", \"Û\", text)\n",
    "    text = re.sub(r\"&Ugrave;\", \"Ù\", text)\n",
    "    text = re.sub(r\"&Uuml;\", \"U\", text)\n",
    "    text = re.sub(r\"&Ccedil;\", \"Ç\", text)\n",
    "    text = re.sub(r\"&Ntilde;\", \"Não\", text)\n",
    "    text = re.sub(r\"&Yacute;\", \"Y\", text)\n",
    "\n",
    "    #any special char to be replaced\n",
    "    text = re.sub(r\"&lt;\", \"<\", text)\n",
    "    text = re.sub(r\"&gt;\", \">\", text)\n",
    "    text = re.sub(r\"&amp;\", \"&\", text)\n",
    "    text = re.sub(r\"&reg;\", \"&\", text) \n",
    "    text = re.sub(r\"&copy;\", \"&\", text) \n",
    "    text = re.sub(r\"&quot;\",\"'\", text)\n",
    "    text = re.sub(r\"&lsquo;\",\"'\", text)\n",
    "    text = re.sub(r\"&rsquo;\",\"'\", text)\n",
    "    return text\n",
    "\n",
    "# Termos em code HTML\n",
    "def _remove_char_html(text):\n",
    "    text = re.sub(r\"&ndash;\",\"\", text)\n",
    "    text = re.sub(r\"&ordf;\",\"\", text)\n",
    "    text = re.sub(r\"&nbsp;\",\"\", text)\n",
    "    text = re.sub(r\"&cent;\",\"\", text)\n",
    "    text = re.sub(r\"&pound;\",\"\", text)\n",
    "    text = re.sub(r\"&sect;\",\"\", text)\n",
    "    text = re.sub(r\"&copy;\",\"\", text)\n",
    "    text = re.sub(r\"&laquo;\",\"\", text)\n",
    "    text = re.sub(r\"&raquo;\",\"\", text)\n",
    "    text = re.sub(r\"&reg;\",\"\", text)\n",
    "    text = re.sub(r\"&deg;\",\"\", text)\n",
    "    text = re.sub(r\"&plusmn;\",\"\", text)\n",
    "    text = re.sub(r\"&para;\",\"\", text)\n",
    "    text = re.sub(r\"&middot;\",\"\", text)\n",
    "    text = re.sub(r\"&frac12;\",\"\", text)\n",
    "    text = re.sub(r\"&mdash;\",\"\", text)\n",
    "    text = re.sub(r\"&lsquo;\",\"\", text)\n",
    "    text = re.sub(r\"&rsquo;\",\"\", text)\n",
    "    text = re.sub(r\"&sbquo;\",\"\", text)\n",
    "    text = re.sub(r\"&ldquo;\",\"\", text)\n",
    "    text = re.sub(r\"&rdquo;\",\"\", text)\n",
    "    text = re.sub(r\"&bdquo;\",\"\", text)\n",
    "    text = re.sub(r\"&dagger;\",\"\", text)\n",
    "    text = re.sub(r\"&Dagger;\",\"\", text)\n",
    "    text = re.sub(r\"&bull;\",\"\", text)\n",
    "    text = re.sub(r\"&hellip;\",\"\", text)\n",
    "    text = re.sub(r\"&prime;\",\"\", text)\n",
    "    text = re.sub(r\"&Prime;\",\"\", text)\n",
    "    text = re.sub(r\"&euro;\",\"\", text)\n",
    "    text = re.sub(r\"&trade;\",\"\", text)\n",
    "    text = re.sub(r\"&asymp;\",\"\", text)\n",
    "    text = re.sub(r\"&ne;\",\"\", text)\n",
    "    text = re.sub(r\"&le;\",\"\", text)\n",
    "    text = re.sub(r\"&ge;\",\"\", text)\n",
    "    text = re.sub(r\"&lt;\",\"\", text)\n",
    "    text = re.sub(r\"&gt;\",\"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text(text):\n",
    "    text = _remove_tags_html(text)\n",
    "    text = _remove_email(text)\n",
    "    text = _remove_emoji(text)\n",
    "    text = _sub_char_especial_normal(text)\n",
    "    text = _remove_char_html(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando a transformação nas reviews\n",
    "imdb['review']=imdb['review'].apply(str)\n",
    "imdb['review_transform'] = imdb['review'].apply(transform_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Map labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {  'positive':1,\n",
    "                'negative':0}\n",
    "\n",
    "imdb['sentiment_num'] = imdb['sentiment'].map(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_num</th>\n",
       "      <th>review_transform</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>A wonderful little production. The filming tec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  sentiment_num  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive              1   \n",
       "1  A wonderful little production. <br /><br />The...  positive              1   \n",
       "2  I thought this was a wonderful way to spend ti...  positive              1   \n",
       "3  Basically there's a family where a little boy ...  negative              0   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive              1   \n",
       "\n",
       "                                    review_transform  \n",
       "0  One of the other reviewers has mentioned that ...  \n",
       "1  A wonderful little production. The filming tec...  \n",
       "2  I thought this was a wonderful way to spend ti...  \n",
       "3  Basically there's a family where a little boy ...  \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = imdb[['review_transform','sentiment_num']].iloc[:25000]\n",
    "test_data = imdb[['review_transform','sentiment_num']].iloc[25000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentences = []\n",
    "training_labels = []\n",
    "\n",
    "testing_sentences = []\n",
    "testing_labels = []\n",
    "\n",
    "for l in train_data['sentiment_num']:\n",
    "    training_labels.append(l)\n",
    "for s in train_data['review_transform']:\n",
    "    training_sentences.append(str(s))\n",
    "\n",
    "for l in test_data['sentiment_num']:\n",
    "    testing_labels.append(l)\n",
    "for s in test_data['review_transform']:\n",
    "    testing_sentences.append(str(s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels_final = np.array(training_labels)\n",
    "testing_labels_final = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "embeding_dim = 16\n",
    "max_lenght = 1000\n",
    "trunc_type = 'post'\n",
    "oov_tok = 'XXXXXX'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(  num_words=vocab_size,\n",
    "                        oov_token= oov_tok)\n",
    "\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "padded = pad_sequences( sequences, \n",
    "                        maxlen=max_lenght,\n",
    "                        truncating = trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens do vocabulário no exame de teste.\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences( testing_sequences,\n",
    "                                maxlen = max_lenght)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Model - Rede Neural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Construindo rede neural - Model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embeding_dim, input_length = max_lenght), # Aqui é onde a magia realmente acontece\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(6, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(  loss = 'binary_crossentropy',\n",
    "                optimizer = 'adam',\n",
    "                metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 1000, 16)          160000    \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 16000)             0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 6)                 96006     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256,013\n",
      "Trainable params: 256,013\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 13s 14ms/step - loss: 0.4316 - accuracy: 0.7746 - val_loss: 0.2810 - val_accuracy: 0.8831\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 0.1751 - accuracy: 0.9354 - val_loss: 0.2901 - val_accuracy: 0.8831\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 0.0732 - accuracy: 0.9797 - val_loss: 0.3304 - val_accuracy: 0.8826\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 0.0235 - accuracy: 0.9963 - val_loss: 0.3921 - val_accuracy: 0.8804\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 0.0070 - accuracy: 0.9993 - val_loss: 0.4312 - val_accuracy: 0.8786\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.0023 - accuracy: 0.9998 - val_loss: 0.4903 - val_accuracy: 0.8780\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 8.0904e-04 - accuracy: 1.0000 - val_loss: 0.5285 - val_accuracy: 0.8796\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 4.1366e-04 - accuracy: 1.0000 - val_loss: 0.5586 - val_accuracy: 0.8793\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 2.3618e-04 - accuracy: 1.0000 - val_loss: 0.5874 - val_accuracy: 0.8788\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 1.4180e-04 - accuracy: 1.0000 - val_loss: 0.6176 - val_accuracy: 0.8791\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(  padded,\n",
    "            training_labels_final,\n",
    "            epochs = num_epochs,\n",
    "            validation_data = (testing_padded, testing_labels_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Construindo rede Neural - Model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embeding_dim, input_length = max_lenght), # Aqui é onde a magia realmente acontece\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(6, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.compile(  loss = 'binary_crossentropy',\n",
    "                optimizer = 'adam',\n",
    "                metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 1000, 16)          160000    \n",
      "                                                                 \n",
      " global_average_pooling1d_1   (None, 16)               0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 6)                 102       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,109\n",
      "Trainable params: 160,109\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "79/79 [==============================] - 5s 52ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6928 - val_accuracy: 0.4997\n",
      "Epoch 2/10\n",
      "79/79 [==============================] - 4s 55ms/step - loss: 0.6923 - accuracy: 0.5132 - val_loss: 0.6916 - val_accuracy: 0.4997\n",
      "Epoch 3/10\n",
      "79/79 [==============================] - 4s 53ms/step - loss: 0.6903 - accuracy: 0.5100 - val_loss: 0.6896 - val_accuracy: 0.5011\n",
      "Epoch 4/10\n",
      "79/79 [==============================] - 4s 53ms/step - loss: 0.6862 - accuracy: 0.5632 - val_loss: 0.6850 - val_accuracy: 0.5190\n",
      "Epoch 5/10\n",
      "79/79 [==============================] - 4s 52ms/step - loss: 0.6774 - accuracy: 0.5884 - val_loss: 0.6741 - val_accuracy: 0.7453\n",
      "Epoch 6/10\n",
      "79/79 [==============================] - 4s 46ms/step - loss: 0.6583 - accuracy: 0.7244 - val_loss: 0.6557 - val_accuracy: 0.7283\n",
      "Epoch 7/10\n",
      "79/79 [==============================] - 4s 48ms/step - loss: 0.6306 - accuracy: 0.7660 - val_loss: 0.6304 - val_accuracy: 0.7724\n",
      "Epoch 8/10\n",
      "79/79 [==============================] - 4s 54ms/step - loss: 0.5944 - accuracy: 0.7968 - val_loss: 0.6061 - val_accuracy: 0.7175\n",
      "Epoch 9/10\n",
      "79/79 [==============================] - 4s 54ms/step - loss: 0.5519 - accuracy: 0.8424 - val_loss: 0.5732 - val_accuracy: 0.7665\n",
      "Epoch 10/10\n",
      "79/79 [==============================] - 5s 58ms/step - loss: 0.5074 - accuracy: 0.8700 - val_loss: 0.5440 - val_accuracy: 0.7707\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f7e2ba5130>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.fit(  padded,\n",
    "            training_labels_final,\n",
    "            epochs = num_epochs,\n",
    "            validation_data = (testing_padded, testing_labels_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "# Obtendo os resultados da camada de embedding (primeira camada)\n",
    "e = model.layers[0]\n",
    "weights = e.get_weights()[0]\n",
    "print(weights.shape) # Shape: (Vocab_size, embedding_dim)\n",
    "# 10000 Palavras e 16 dimensões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para sermos capazes de plotar as palavras como vetores, devemos o índice de palavras\n",
    "reverse_word_index = dict([(value, key) for (key,value) in word_index.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Escrevendo os vetores em formatos de metadados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word_num in range(1, vocab_size):\n",
    "    word = reverse_word_index[word_num]\n",
    "    embeddings = weights[word_num]\n",
    "    out_m.write(word + '\\n')\n",
    "    out_v.write('\\t'.join([str(x) for x in embeddings]) + '\\n')\n",
    "out_m.close()\n",
    "out_v.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O projecto TensorFlow lê esse tipo de arquivo e usa os dados para plotar um gráfico em 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3ee7205dc1c228b6cd913c2a156bb10dc2778a82715d275e9a46fb84fa02867e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('curso_nlp': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
